\startchapter{Technical Relations and Failure}
\label{chap:tech-net}
With this chapter we survey the rich history in failure predicting to find evidence that technical networks, constructed by project members connected through source code artifatcs, can predict software quality.
We start with detailing our methodology to conduct our literature review (Section~\ref{chap:6:methods}), followed by three sections detailing our findings roughly categorized into: 
(1) work that leverages metrics hinting at technical networks (Section~\ref{chap:6:measure}),
(2) work that is based on software artifact networks (Section~\ref{chap:6:an}),
and (3) work that leverages technical networks (Section~\ref{chap:6:tn}).
And we close this chapter with a discussion of our findings (Section~\ref{chap:6:dis}).

\section{Methods}
\label{chap:6:methods}
% venues - ISESE, WCRE, ICSM, SCAM, RSSE, ESEM, Metrics
\begin{table}[t]
\centering
\begin{tabular}{lrrr}
\toprule
Venue Name & Venue Type & \#Papers & Vanue Size\\
\midrule
(TSE) IEEE Transactions of Software Engineering & Journal &\\
IEEE Software & Jounral & \\
(ICSE) International Conference on Software Engineering & Proceedings & \\
(FSE) Symposium on the Foundations of Software Engineering & Proccedings & \\
(MSR) Working Conference on Mining software Repositories & Proceedings & \\
(ISESE) International Symposium on Empirical Software Engineering & Proceedings & \\
(WCRE) Working Conference on Reverse Engineering & Proceedings & \\
(ICSM) International Conference on Software Maintenance & Proceedings & \\
(RSSE) International Workshop on Recommender Systems in Software Engineering & Proceedings & \\
(ESEM) International Symposium on Emprirical Engineering and Measurment & Proceedings & \\
(METRICS) International Software Metrics Symposium & Proceedings & \\
(PROMISE) & Proceedings & \\
(ASE) & Proceedings & \\
\bottomrule
\end{tabular}
\label{chap:6:tab:venues}
\caption{Venues used as basis for the systematic literature review.}
\end{table}

We conduct a systematic literature review as proposed by ...~\cite{} and proceed in the following seven steps:

\begin{enumerate}
\item Define research question that the review is supposed to answer.
\item Define search terms.
\item Select relevant venues.
\item Read abstracts of studies that fit search terms.
\item Add paper to repository if adequate.
\item Go through repository and apply search to paper references.
\item Create categories from papers.
\end{enumerate}

The research questions guiding the systematic review is aimed at identifying a connection between technical networks and software quality.

\begin{note}
\emph{RQ:} Can technical network be related to software quality? 
\end{note}

% why do we use a lit review
Due to the history of software engineering research with respect to predicting and finding indicators of software quality, we suspect that enough research has been conducted that shows ample evidence of or against the existence of a relationship between software quality and technical networks.

\begin{table}
\centering
\begin{tabular}{ll}
Terms & \\
\end{tabular}
\label{chap:6:terms}
\caption{exhaustive list of search terms used to identify relevant papers for the systematic review.}
\end{table}

\paragraph{Search Terms.} Fromn the research question we extracted two sets of search terms.
One set conists of terms related to software quality such as bugs, failure and the other set contains terms that relate to software measures and networks of constructed using software artifacts.
For a detailed list of search teams please refer to Table~\ref{chap:6:terms}

\paragraph{Relevant venues.} We selected relevant venues as listed in Table~\ref{} based on their relevance to the field of software engineering at large or with respect to the data sources usually employed in the field of failure prediction using software artifacts.
This selection lead us to a copus of XXX papers before selecting relevant papers.

\paragraph{Reading abstracts to add studies to list of relevant papers.} After applying the search terms to narrow down the search from the list of all papers in our venues of interest we proceed with readin all abstracts of the remaining papers to decide whether the paper is relevabt to our literature review.
This left us with a corpus of XXX papers.

\paragraph{Related work.} After we selected all relevant papers from our initial corpus, we repeat the same process using all referenced work in our corpus of interest until there is no more work found that is relevant to our literature review.
We applied this XXX times until the referenced work of our corpus of interest did not cite any work of relevance to us and ended up with a corpus comtaining XXX papers.

\paragraph{Categories.} From the sutdies we surveyed we extracted three categories that roughly span all the relvant surveyed studies:
(1) measures studied in relation to software failures that are implicilty based on technical networks,
(2) studies that explicily create network from software artifacts and related them to software failures,
and (3) studies that relate technical networks to software failres.
Note that a study can be part of more than one category.

\section{Technical Network Related Measures}
\label{chap:6:measure}
Software artifact networks are necessarily made explicit to compute certain metrics but they can be often found as the implicit foundations of those metrics.
We assign all of the XXX studies of our survey that fall into the category of using measure related to software artifact derived networks into three sub-categories:
(1) code complexity metrics often measure the interdependencies between low level software artifacts,
(2) object oriented metrics often measure very localized aspects or the class interdependecies,
and (3) metrics that measure code interdependcies such as function fan in and fan out.

\subsection{Code Complexity Metrics}
Defect prediction heavily uses metrics the describe code to predict defect proness of files and other levels of software artifacts.
One such type of code metric is assessing the complexity of code.
Code complexity related to technical networks as a proxy.
Since most complexity is derived from interactio between software artifacts or parts of software artifacts they implicitly measure the compelxity of a technical network.

The two most common complexity metrics used are Lines of Code in a software artifact and Mc Cabe's complexity metric~\cite{}.
Both metrics have been found to exhibit a medium to strong correlation across various software projects.
In principal, source code files or methods with more code lines have a higher changes to describe a more complex data flow.
Similarly, both metric indicate that the more complex code is the more relations it has to other parts of the code.

The first study undertaken to show the relation between multiple code metrics including lines of code and other metric such as Mc Cabe's complexity as well as other metrics, such as fan in and fan out which we will cover in Section~\ref{}, was undertaken by Nagappan et al~\cite{nagappan:icse:2006} at Microsoft.
Both lines of code and Mc Cabe's complexity showed a correlation to post-release failures per file, failures that are reported by the customer within the first six months after release, of medium strength.

Although we consider Nagappan at al work to be among the first comprehensive studies into predicting post-release defects using multople source code metrics, other studies the relation ship of particular metrics with defect density.
For instance, Koru et al~\cite{koru:promise:2005} conducted an indepth anaylisis of the relation between the number of lines of code within a file and the relationship to defect density.
Size not only is a defect predictor in many forms, e.g., average function size or file size, but the size of components also determines how well other predictors can work on it.

In 2005 one year before Nagappan et al published their study on the relation of code metrics to defect density, Nikora et al~\cite{nikora:isese:2006} discussed the use of graph masures used on the control flow graph.
In the light of Mc Cabe's complexity metric that gives one value to the overall control flow graph within a software artifact, Nikora et al characterisze the control flow graph using graph measures such as ...

Complexity and size metrics are often used as a control variable to ensure that newly found metrics are not simply a new representation of size or complexity and actually add more predictive power to already known metrics.
Hence, there is ample evidence that lines of code~\cite{shihab:esem:2010,arisholm:isese:2006,jiang:promise:2008,knab:msr:2006,zhang:icsm:2009} and Mc Cabe's complexity~\cite{nagappan:icse:2006,shihab:fse:2011,zimermann:fse:2009,jiang:promise:2008,zimmermann:promise:2207} have a moderate correlation to software defects on a file level.

Despite their moderate usefulness complexity mesures as we know them so far especially lines of code and Mc Cabe's complexity metric do not generalize across projects.
This means, using those metrics as predictors adjusted to one project usually offers poor predictions when applied to another~\cite{zimmermann:fse:2009}. 


\subsection{Change Complexity Metrics}
Besides measureing the complexity of the source code artifacts, measuring the complexity of the development of those code artifacts can also hint at dependencies across artifacts.
During the evolution or maintenance of a project files are continously edited to extend or improve the exiting project.
These changes implicitly change the dependencies to other software artifacts by changing call and data dependencoes and thus construct and measure an implicit software artifact network.
Furthermore, changing files can create dependencies among the owner of the modified or affected and thus construct a technical network.

Softwre artifact modifications are seldonly equally distributed across the project time and the artifacts related to a project.
Thus, software artifact changes lend themselves to be studies in relation to software quality.
The simplest metric to measure on a file level is to count the amount of change that happened to a file within a given time frame~\cite{li:metrics:2005i,moser:icse:2008,cataldo:icse:2011}.

The next logical step after investigating the amount of change is to chareterise the complexity of the changes made to a software artifact. 
These chrun metric measure similarly to lines of code the distribution of the change size in a given time frame~\cite{nagappan:icse:2005,shihab:fse:2011,zimmermann:fse:2009,bell:promise:2011}.
To take it one step further compared to studying the actual change sizes per software artifact the change of the change sizes poses a good predictor on the grounds that changes that seem out of the ordinary are more likely to introduce issues~\cite{hassan:icse:2009}.
Instead of looking into high level artifacts investigating low level changes on the level of control flow instructions such as if-then-else clauses also yields a good failure predictor~\cite{giger:msr:2011}.

Similarly to the code complexity metrics, change complecity metrics are also not necessarily able to predict failure density for a project when trained with data from a different projects~\cite{zimmermann:fse:2009}.


\subsection{Object Oriented Metrics}
Object oriented metrics are taking into account more language specific construct that in the caase of object oriented programming are often describing the relationship between object, such as inheritance depth, and thus desribing an implicit network constructed from objetc relationships.
Besides mesureing the relationships between objects object oriented metrics also measure the complexity of objects using metrics such as method counts.

Complexity mesures in object oriented metrics are similarly relating to technical networks as explained in the previous section as code and change complexity.
Method counts have been used by Nagappan et al~\cite{nagappan:icse:2006} and Arisholm et al~\cite{arisholm:isese:2006} showed a moderatly strong correlation between the amount of methods within a class and both post-release failures in Windows Vista and the defect density of components in telecumincation softwre.

Direct mesures of object relationship, such as inhertance depth~\cite{}, sub-classing~\cite{} and object coupling~\cite{}, can predict software quality~\cite{nagappan:icse:2006,arisholm:isese:2006,english:promise:2009}.
In light of our research question, whether technical-networks can predict software failures, the relationship between measures of object relationships and defect density are a direct indicator for the ability of technical networks to predict software quality.


\subsection{Dependency Metrics}
Besides the three types of metrics discussed that measure complexity of code and in the case of object oriented metrics sometimes actual dependencies between software artifacts.
Metrics such as fan-in and fan-out~\cite{} of methods are directly measureing the dependencies between software artifacts and thus giving a clearer picture of the underlying technical network.

Counting the number of dependecies either through metrics such as fan-in or fan-out or straight up counting call dependencies between software artifacts already yield moderate failure predictors~\cite{cataldo:icse:2011,nagappan:icse:2006,arisholm:isese:2006,knab:msr:2006,shin:msr:2006}.
Note that although most of the object oriented metrics we mentioned in the previous section (Section~\ref{}) are also dependecy metrics we will not reiterate them in this section. 
Using more course graiined dependency on the modul level, or in other words counting dependencies across modul boundaies, can also yield good failure predictors~\cite{jiang:promise:2008}.
Another dependecy metric creating dependencies between files or larger modules, such as Java packages, uses the usage relationship between software artifacts as they can be defined at the time of a software projects design~\cite{schoeter:isese:2006,dualaekoko:esem:2009}.

Earlier we showed that the definition of complexity used on software artifacts such as source code files can also be extended to the notion of change complexity such as code churn, similarly dependecies between source code artifacts can be inferd using change information.
Zimmermann et al~\cite{zimmermann:icse:2004} and D'Ambros et al~\cite{dambros:wcre:2009} used the informaiton about co-chaning files, that are files that are frequently changed togehter in the same change-set, to determine how dangarous it can be to not change those files togethr.
Zimmermann et al's work originally was inteeded to reommend with files to additionally change to an original software change  found that violating those co-change dependecies can lead to software failure.
Build on that work D'Ambros et al used the amount of those vialations to build a prediction model that is able to predict defect lieklihood for a file in a given project release. 


\section{Artifact Networks}
\label{chap:6:an}

- fixes occur at code peers~\cite{nguyen:icse:2010} 
- ego network measures on data and call dependencies build for one instance of program~\cite{zimmermann:icse:2008}
- ego binary netowrk can predict failure likelihood of binary~\cite{zimmermann:esem:2009}

\section{Technical Networks}
\label{chap:6:tn}
file failure:
- co changed files networks~\cite{meneely:fse:2008}

binary failure:
- ownership relations by changed same binary~\cite{pinzger:fse:2008}


\section{Discussion}
\label{chap:6:dis}
