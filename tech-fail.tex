\startchapter{Technical Relations and Failure}
\label{chap:tech-net}
With this chapter we survey the rich history in failure predicting to find evidence that technical networks, constructed by project members connected through source code artifatcs, can predict software quality.
We start with detailing our methodology to conduct our literature review (Section~\ref{chap:6:methods}), followed by three sections detailing our findings roughly categorized into: 
(1) work that leverages metrics hinting at technical networks (Section~\ref{chap:6:measure}),
(2) work that is based on software artifact networks (Section~\ref{chap:6:an}),
and (3) work that leverages technical networks (Section~\ref{chap:6:tn}).
And we close this chapter with a discussion of our findings (Section~\ref{chap:6:dis}).

\section{Methods}
\label{chap:6:methods}
% venues - ISESE, WCRE, ICSM, SCAM, RSSE, ESEM, Metrics
\begin{table}[t]
\centering
\begin{tabular}{lrrr}
\toprule
Venue Name & Venue Type & \#Papers & Vanue Size\\
\midrule
(TSE) IEEE Transactions of Software Engineering & Journal &\\
IEEE Software & Jounral & \\
(ICSE) International Conference on Software Engineering & Proceedings & \\
(FSE) Symposium on the Foundations of Software Engineering & Proccedings & \\
(MSR) Working Conference on Mining software Repositories & Proceedings & \\
(ISESE) International Symposium on Empirical Software Engineering & Proceedings & \\
(WCRE) Working Conference on Reverse Engineering & Proceedings & \\
(ICSM) International Conference on Software Maintenance & Proceedings & \\
(RSSE) International Workshop on Recommender Systems in Software Engineering & Proceedings & \\
(ESEM) International Symposium on Emprirical Engineering and Measurment & Proceedings & \\
(METRICS) International Software Metrics Symposium & Proceedings & \\
(PROMISE) & Proceedings & \\
(ASE) & Proceedings & \\
\bottomrule
\end{tabular}
\label{chap:6:tab:venues}
\caption{Venues used as basis for the systematic literature review.}
\end{table}

We conduct a systematic literature review as proposed by ...~\cite{} and proceed in the following seven steps:

\begin{enumerate}
\item Define research question that the review is supposed to answer.
\item Define search terms.
\item Select relevant venues.
\item Read abstracts of studies that fit search terms.
\item Add paper to repository if adequate.
\item Go through repository and apply search to paper references.
\item Create categories from papers.
\end{enumerate}

The research questions guiding the systematic review is aimed at identifying a connection between technical networks and software quality.

\begin{note}
\emph{RQ:} Can technical network be related to software quality? 
\end{note}

% why do we use a lit review
Due to the history of software engineering research with respect to predicting and finding indicators of software quality, we suspect that enough research has been conducted that shows ample evidence of or against the existence of a relationship between software quality and technical networks.

\begin{table}
\centering
\begin{tabular}{ll}
Terms & \\
\end{tabular}
\label{chap:6:terms}
\caption{exhaustive list of search terms used to identify relevant papers for the systematic review.}
\end{table}

\paragraph{Search Terms.} Fromn the research question we extracted two sets of search terms.
One set conists of terms related to software quality such as bugs, failure and the other set contains terms that relate to software measures and networks of constructed using software artifacts.
For a detailed list of search teams please refer to Table~\ref{chap:6:terms}

\paragraph{Relevant venues.} We selected relevant venues as listed in Table~\ref{} based on their relevance to the field of software engineering at large or with respect to the data sources usually employed in the field of failure prediction using software artifacts.
This selection lead us to a copus of XXX papers before selecting relevant papers.

\paragraph{Reading abstracts to add studies to list of relevant papers.} After applying the search terms to narrow down the search from the list of all papers in our venues of interest we proceed with readin all abstracts of the remaining papers to decide whether the paper is relevabt to our literature review.
This left us with a corpus of XXX papers.

\paragraph{Related work.} After we selected all relevant papers from our initial corpus, we repeat the same process using all referenced work in our corpus of interest until there is no more work found that is relevant to our literature review.
We applied this XXX times until the referenced work of our corpus of interest did not cite any work of relevance to us and ended up with a corpus comtaining XXX papers.

\paragraph{Categories.} From the sutdies we surveyed we extracted three categories that roughly span all the relvant surveyed studies:
(1) measures studied in relation to software failures that are implicilty based on technical networks,
(2) studies that explicily create network from software artifacts and related them to software failures,
and (3) studies that relate technical networks to software failres.
Note that a study can be part of more than one category.

\section{Technical Network Related Measures}
\label{chap:6:measure}
Software artifact networks are necessarily made explicit to compute certain metrics but they can be often found as the implicit foundations of those metrics.
We assign all of the XXX studies of our survey that fall into the category of using measure related to software artifact derived networks into three sub-categories:
(1) code complexity metrics often measure the interdependencies between low level software artifacts,
(2) object oriented metrics often measure very localized aspects or the class interdependecies,
and (3) metrics that measure code interdependcies such as function fan in and fan out.

\subsection{Code Complexity Metrics}
Defect prediction heavily uses metrics the describe code to predict defect proness of files and other levels of software artifacts.
One such type of code metric is assessing the complexity of code.
Code complexity related to technical networks as a proxy.
Since most complexity is derived from interactio between software artifacts or parts of software artifacts they implicitly measure the compelxity of a technical network.

The two most common complexity metrics used are Lines of Code in a software artifact and Mc Cabe's complexity metric~\cite{mccabe:ieee:1976}.
Both metrics have been found to exhibit a medium to strong correlation across various software projects.
In principal, source code files or methods with more code lines have a higher changes to describe a more complex data flow.
Similarly, both metric indicate that the more complex code is the more relations it has to other parts of the code.

The first study undertaken to show the relation between multiple code metrics including lines of code and other metric such as Mc Cabe's complexity as well as other metrics, such as fan in and fan out which we will cover in Section~\ref{chap:6:sub:depmet}, was undertaken by Nagappan et al~\cite{nagappan:icse:2006} at Microsoft.
Both lines of code and Mc Cabe's complexity showed a correlation to post-release failures per file, failures that are reported by the customer within the first six months after release, of medium strength.

Although we consider Nagappan at al work to be among the first comprehensive studies into predicting post-release defects using multople source code metrics, other studies the relation ship of particular metrics with defect density.
For instance, Koru et al~\cite{koru:promise:2005} conducted an indepth anaylisis of the relation between the number of lines of code within a file and the relationship to defect density.
Size not only is a defect predictor in many forms, e.g., average function size or file size, but the size of components also determines how well other predictors can work on it.

In 2005 one year before Nagappan et al published their study on the relation of code metrics to defect density, Nikora et al~\cite{nikora:isese:2006} discussed the use of graph masures used on the control flow graph.
In the light of Mc Cabe's complexity metric that gives one value to the overall control flow graph within a software artifact, Nikora et al characterisze the control flow graph using graph measures such as ...

Complexity and size metrics are often used as a control variable to ensure that newly found metrics are not simply a new representation of size or complexity and actually add more predictive power to already known metrics.
Hence, there is ample evidence that lines of code~\cite{shihab:esem:2010,arisholm:isese:2006,jiang:promise:2008,knab:msr:2006,zhang:icsm:2009} and Mc Cabe's complexity~\cite{nagappan:icse:2006,shihab:fse:2011,zimermann:fse:2009,jiang:promise:2008,zimmermann:promise:2207} have a moderate correlation to software defects on a file level.

Despite their moderate usefulness complexity mesures as we know them so far especially lines of code and Mc Cabe's complexity metric do not generalize across projects.
This means, using those metrics as predictors adjusted to one project usually offers poor predictions when applied to another~\cite{zimmermann:fse:2009}. 


\subsection{Change Complexity Metrics}
Besides measureing the complexity of the source code artifacts, measuring the complexity of the development of those code artifacts can also hint at dependencies across artifacts.
During the evolution or maintenance of a project files are continously edited to extend or improve the exiting project.
These changes implicitly change the dependencies to other software artifacts by changing call and data dependencoes and thus construct and measure an implicit software artifact network.
Furthermore, changing files can create dependencies among the owner of the modified or affected and thus construct a technical network.

Softwre artifact modifications are seldonly equally distributed across the project time and the artifacts related to a project.
Thus, software artifact changes lend themselves to be studies in relation to software quality.
The simplest metric to measure on a file level is to count the amount of change that happened to a file within a given time frame~\cite{li:metrics:2005i,moser:icse:2008,cataldo:icse:2011}.

The next logical step after investigating the amount of change is to chareterise the complexity of the changes made to a software artifact. 
These chrun metric measure similarly to lines of code the distribution of the change size in a given time frame~\cite{nagappan:icse:2005,shihab:fse:2011,zimmermann:fse:2009,bell:promise:2011}.
To take it one step further compared to studying the actual change sizes per software artifact the change of the change sizes poses a good predictor on the grounds that changes that seem out of the ordinary are more likely to introduce issues~\cite{hassan:icse:2009}.
Instead of looking into high level artifacts investigating low level changes on the level of control flow instructions such as if-then-else clauses also yields a good failure predictor~\cite{giger:msr:2011}.

Similarly to the code complexity metrics, change complecity metrics are also not necessarily able to predict failure density for a project when trained with data from a different projects~\cite{zimmermann:fse:2009}.


\subsection{Object Oriented Metrics}
\label{chap:6:sub:oom}
Object oriented metrics are taking into account more language specific construct that in the caase of object oriented programming are often describing the relationship between object, such as inheritance depth, and thus desribing an implicit network constructed from objetc relationships.
Besides mesureing the relationships between objects object oriented metrics also measure the complexity of objects using metrics such as method counts.

Complexity mesures in object oriented metrics are similarly relating to technical networks as explained in the previous section as code and change complexity.
Method counts have been used by Nagappan et al~\cite{nagappan:icse:2006} and Arisholm et al~\cite{arisholm:isese:2006} showed a moderatly strong correlation between the amount of methods within a class and both post-release failures in Windows Vista and the defect density of components in telecumincation softwre.

Direct mesures of object relationship, such as inhertance depth~\cite{chidamber:tse:1994}, sub-classing~\cite{chidamber:tse:1994} and object coupling~\cite{chidamber:tse:1994}, can predict software quality~\cite{nagappan:icse:2006,arisholm:isese:2006,english:promise:2009}.
In light of our research question, whether technical-networks can predict software failures, the relationship between measures of object relationships and defect density are a direct indicator for the ability of technical networks to predict software quality.


\subsection{Dependency Metrics}
\label{chap:6:sub:depmet}
Besides the three types of metrics discussed that measure complexity of code and in the case of object oriented metrics sometimes actual dependencies between software artifacts.
Metrics such as fan-in and fan-out~\cite{henry:tse:1981} of methods are directly measureing the dependencies between software artifacts and thus giving a clearer picture of the underlying technical network.

Counting the number of dependecies either through metrics such as fan-in or fan-out or straight up counting call dependencies between software artifacts already yield moderate failure predictors~\cite{cataldo:icse:2011,nagappan:icse:2006,arisholm:isese:2006,knab:msr:2006,shin:msr:2006}.
Note that although most of the object oriented metrics we mentioned in the previous section (Section~\ref{chap:6:sub:oom}) are also dependecy metrics we will not reiterate them in this section. 
Using more course graiined dependency on the modul level, or in other words counting dependencies across modul boundaies, can also yield good failure predictors~\cite{jiang:promise:2008}.
Another dependecy metric creating dependencies between files or larger modules, such as Java packages, uses the usage relationship between software artifacts as they can be defined at the time of a software projects design~\cite{schoeter:isese:2006,dualaekoko:esem:2009}.

Earlier we showed that the definition of complexity used on software artifacts such as source code files can also be extended to the notion of change complexity such as code churn, similarly dependecies between source code artifacts can be inferd using change information.
Zimmermann et al~\cite{zimmermann:icse:2004} and D'Ambros et al~\cite{dambros:wcre:2009} used the informaiton about co-chaning files, that are files that are frequently changed togehter in the same change-set, to determine how dangarous it can be to not change those files togethr.
Zimmermann et al's work originally was inteeded to reommend with files to additionally change to an original software change  found that violating those co-change dependecies can lead to software failure.
Build on that work D'Ambros et al used the amount of those vialations to build a prediction model that is able to predict defect lieklihood for a file in a given project release. 


\section{Artifact Networks}
\label{chap:6:an}
In the previous Section~\ref{chap:6:measure} we presented evidence of the relation between technical networks and their predictive powers with repsect to software quality by going over past work that uses software metrics implicitly measuring the underlying technical network to predict software defects.
This Section serves the purpose of exploring the exiting work done on failure prediction that consiously leverages networks that are build from linked software artifacts.

Using dependencies within a product one can construct a network of software artifacts that share dependencies among each other.
Artifacts that have direct depdencies are sometimes in the case of source code referred to as code peers.
One interesting property of code peers is that in case a code peer exhibits a defect it increases the likelihood that the code artifact whose peer contains a defect has a higher likelihood to contain a defect by itself~\cite{nguyen:icse:2010}.

Form this notion of a code peer and the influence a code peer or more generally an artifact peer can have on the quality of an artifact, analysing these network with respect to an artifact and it surrounding artifacts.
In a first study Zimmermann et al~\cite{zimmermann:icse:2008} studied data and call dependecy measures a single artifact has to its dependent artifacts and found it to be a good predictor for software defects.

In a follow up study Zimmermann et al~\cite{zimmmermann:esem:2009} extended the influence of an artifacts peer by not solely focusing on an artifacts dependencies to its peers but taking into account the dependencies among an artifacts peers.
This enables the application of network measures and social-network measures to characterize this ego network constructed around a singe software artifacts.
As it turn out, the predictive power of such a network can be more powerful than only considering dependencies between an artifact and its peers~\cite{zimmermann:esem:2009.

\section{Technical Networks}
\label{chap:6:tn}
Although the evidence presented in the previous Section~\ref{chap:6:an} on using measures capturing Artifact Networks already provides ample evidence for the predictive power of technical networks, we are going to present the final peace of evidence to make our argument of answering our research questions by going over research that constructed networks very similar to the technical networks as we defined them in this thesis.

To go from artifact network to technical network developer can be included in the already existing artifact network and thus be represented as a kind of artifact~\cite{pinzger:fse:2008}.
These two mode networks can be used to do the same analysis that Zimmermann et al~\cite{zimmermann:esem:2008,zimmermann:icse:2008} perfomed by focusing on the software artifacts to predict the failure likelihood of each.

Even closer to our definition of a technical network than the description of artifact networks and two mode networks comes the work of Meneely et al~\cite{meneely:fse:2008}.
Their technical networks consits of developer that within a given release modifed the same file, which matches our definition of connecting developer as soon as artifacts they are related to are connected.
These networks, or rather social network measure extracted from these networks, are able to predict the whether a file contain a failure.

\section{Discussion}
\label{chap:6:dis}
In this chapter we strive to answer the first research question of this thesis:
\textbf{RQ 1.1:} Do Technical Networks influence build success?
Based on the extensive literture existing on defect prediction based on technical artifacts, such as source code changes and source code files, we employed a systematic literature review to answer our research question.

The majority of the literure surrounding defect prediction is focus on detecting failures on an artifact level rather than prediction whether a build fails or succeeds.
Considering that a build failure is dependend on a set of test cases and the judgment of developer, whether the build does not contain any unexpeted issues, defects with repect to software quality are generally the issue for a build to be considered a failure.
Given the evidence of the predictability of the failures on component, file and change level combined with the edivende of the last two Sections~\ref{chap:6:an} and~\ref{chap:6:tn} showing the predictive power of artifact network and especially in the case of Meneely et al's work~\cite{meneely:fse:2008} the predictive power of technical networks as we defined them in this thesis.

Since a build can fail either due to some errors that exits within the programs functionality, either discovered through automated or manual testing, or that the program cannot be build or executed, we can answer our research question with yes.
Although the evidence we presented through this chapter does not cover the cases of building problems due to compile or packaging issues, but focus solely on defects withing software artifacts, we conclude that technical networks we can predict a part of the actual build failures that can be prevented through developer intervention.







