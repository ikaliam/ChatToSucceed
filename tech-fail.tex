\startchapter{Technical Relations and Failure}
\label{chap:tech-net}
With this chapter we survey the rich history in failure predicting to find evidence that technical networks, constructed by project members connected through source code artifatcs, can predict software quality.
We start with detailing our methodology to conduct our literature review (Section~\ref{chap:6:methods}), followed by three sections detailing our findings roughly categorized into: 
(1) work that leverages metrics hinting at technical networks (Section~\ref{chap:6:measure}),
(2) work that is based on software artifact networks (Section~\ref{chap:6:an}),
and (3) work that leverages technical networks (Section~\ref{chap:6:tn}).
And we close this chapter with a discussion of our findings (Section~\ref{chap:6:dis}).

\section{Methods}
\label{chap:6:methods}
% venues - ISESE, WCRE, ICSM, SCAM, RSSE, ESEM, Metrics
\begin{table}[t]
\centering
\begin{tabular}{lrrr}
\toprule
Venue Name & Venue Type & \#Papers & Vanue Size\\
\midrule
(TSE) IEEE Transactions of Software Engineering & Journal &\\
IEEE Software & Jounral & \\
(ICSE) International Conference on Software Engineering & Proceedings & \\
(FSE) Symposium on the Foundations of Software Engineering & Proccedings & \\
(MSR) Working Conference on Mining software Repositories & Proceedings & \\
(ISESE) International Symposium on Empirical Software Engineering & Proceedings & \\
(WCRE) Working Conference on Reverse Engineering & Proceedings & \\
(ICSM) International Conference on Software Maintenance & Proceedings & \\
(RSSE) International Workshop on Recommender Systems in Software Engineering & Proceedings & \\
(ESEM) International Symposium on Emprirical Engineering and Measurment & Proceedings & \\
(METRICS) International Software Metrics Symposium & Proceedings & \\
(PROMISE) & Proceedings & \\
(ASE) & Proceedings & \\
\bottomrule
\end{tabular}
\label{chap:6:tab:venues}
\caption{Venues used as basis for the systematic literature review.}
\end{table}

We conduct a systematic literature review as proposed by ...~\cite{} and proceed in the following seven steps:

\begin{enumerate}
\item Define research question that the review is supposed to answer.
\item Define search terms.
\item Select relevant venues.
\item Read abstracts of studies that fit search terms.
\item Add paper to repository if adequate.
\item Go through repository and apply search to paper references.
\item Create categories from papers.
\end{enumerate}

The research questions guiding the systematic review is aimed at identifying a connection between technical networks and software quality.

\begin{note}
\emph{RQ:} Can technical network be related to software quality? 
\end{note}

% why do we use a lit review
Due to the history of software engineering research with respect to predicting and finding indicators of software quality, we suspect that enough research has been conducted that shows ample evidence of or against the existence of a relationship between software quality and technical networks.

\begin{table}
\centering
\begin{tabular}{ll}
Terms & \\
\end{tabular}
\label{chap:6:terms}
\caption{exhaustive list of search terms used to identify relevant papers for the systematic review.}
\end{table}

\paragraph{Search Terms.} Fromn the research question we extracted two sets of search terms.
One set conists of terms related to software quality such as bugs, failure and the other set contains terms that relate to software measures and networks of constructed using software artifacts.
For a detailed list of search teams please refer to Table~\ref{chap:6:terms}

\paragraph{Relevant venues.} We selected relevant venues as listed in Table~\ref{} based on their relevance to the field of software engineering at large or with respect to the data sources usually employed in the field of failure prediction using software artifacts.
This selection lead us to a copus of XXX papers before selecting relevant papers.

\paragraph{Reading abstracts to add studies to list of relevant papers.} After applying the search terms to narrow down the search from the list of all papers in our venues of interest we proceed with readin all abstracts of the remaining papers to decide whether the paper is relevabt to our literature review.
This left us with a corpus of XXX papers.

\paragraph{Related work.} After we selected all relevant papers from our initial corpus, we repeat the same process using all referenced work in our corpus of interest until there is no more work found that is relevant to our literature review.
We applied this XXX times until the referenced work of our corpus of interest did not cite any work of relevance to us and ended up with a corpus comtaining XXX papers.

\paragraph{Categories.} From the sutdies we surveyed we extracted three categories that roughly span all the relvant surveyed studies:
(1) measures studied in relation to software failures that are implicilty based on technical networks,
(2) studies that explicily create network from software artifacts and related them to software failures,
and (3) studies that relate technical networks to software failres.
Note that a study can be part of more than one category.

\section{Technical Network Related Measures}
\label{chap:6:measure}
Software artifact networks are necessarily made explicit to compute certain metrics but they can be often found as the implicit foundations of those metrics.
We assign all of the XXX studies of our survey that fall into the category of using measure related to software artifact derived networks into three sub-categories:
(1) code complexity metrics often measure the interdependencies between low level software artifacts,
(2) object oriented metrics often measure very localized aspects or the class interdependecies,
and (3) metrics that measure code interdependcies such as function fan in and fan out.

\subsection{Code Complexity Metrics}
Defect prediction heavily uses metrics the describe code to predict defect proness of files and other levels of software artifacts.
One such type of code metric is assessing the complexity of code.
Code complexity related to technical networks as a proxy.
Since most complexity is derived from interactio between software artifacts or parts of software artifacts they implicitly measure the compelxity of a technical network.

The two most common complexity metrics used are Lines of Code in a software artifact and Mc Cabe's complexity metric~\cite{}.
Both metrics have been found to exhibit a medium to strong correlation across various software projects.
In principal, source code files or methods with more code lines have a higher changes to describe a more complex data flow.
Similarly, both metric indicate that the more complex code is the more relations it has to other parts of the code.

The first study undertaken to show the relation between multiple code metrics including lines of code and other metric such as Mc Cabe's complexity as well as other metrics, such as fan in and fan out which we will cover in Section~\ref{}, was undertaken by Nagappan et al~\cite{nagappan:icse:2006} at Microsoft.
Both lines of code and Mc Cabe's complexity showed a correlation to post-release failures per file, failures that are reported by the customer within the first six months after release, of medium strength.

Although we consider Nagappan at al work to be among the first comprehensive studies into predicting post-release defects using multople source code metrics, other studies the relation ship of particular metrics with defect density.
For instance, Koru et al~\cite{koru:promise:2005} conducted an indepth anaylisis of the relation between the number of lines of code within a file and the relationship to defect density.
Size not only is a defect predictor in many forms, e.g., average function size or file size, but the size of components also determines how well other predictors can work on it.

In 2005 one year before Nagappan et al published their study on the relation of code metrics to defect density, Nikora et al~\cite{nikora:isese:2006} discussed the use of graph masures used on the control flow graph.
In the light of Mc Cabe's complexity metric that gives one value to the overall control flow graph within a software artifact, Nikora et al characterisze the control flow graph using graph measures such as ...

Complexity and size metrics are often used as a control variable to ensure that newly found metrics are not simply a new representation of size or complexity and actually add more predictive power to already known metrics.
Hence, there is ample evidence that lines of code~\cite{shihab:esem:2010,arisholm:isese:2006,jiang:promise:2008,knab:msr:2006,zhang:icsm:2009} and Mc Cabe's complexity~\cite{nagappan:icse:2006,shihab:fse:2011,zimermann:fse:2009,jiang:promise:2008,zimmermann:promise:2207} have a moderate correlation to software defects on a file level.

Despite their moderate usefulness complexity mesures as we know them so far especially lines of code and Mc Cabe's complexity metric do not generalize across projects.
This means, using those metrics as predictors adjusted to one project usually offers poor predictions when applied to another~\cite{zimmermann:fse:2009}. 


\subsection{Change Complexity Metrics}
integration errors:
- changed lines of code~\cite{cataldo:icse:2011}

file failure:
- changes in changes to low level code fragments~\cite{giger:msr:2011}
- entropy in change complexity~\cite{hassan:icse:2009}
- change metrics line changes/churn~\cite{moser:icse:2008}~\cite{li:metrics:2005}
- code churn~\cite{nagappan:icse:2005}~\cite{shihab:fse:2011} but not across projects~\cite{zimmermann:fse:2009}~\cite{bell:promise:2011}

\subsection{Object Oriented Metrics}
how do they relate to technical networksi

file failure:
- classmethods~\cite{nagappan:icse:2006}~\cite{arisholm:isese:2006}
- inheritance depth, classcoupling, subclasses~\cite{nagappan:icse:2006}~\cite{arisholm:isese:2006}~\cite{english:promise:2009}

\subsection{Dependency Metrics}
how do they relate to technical networks

integration errors:
- number of dependencies~\cite{cataldo:icse:2011}

file failure:
- fanin,fanout,read/write coupling~\cite{nagappan:icse:2006}~\cite{arisholm:isese:2006}~\cite{knab:msr:2006}
- inheritance depth, classcoupling, subclasses~\cite{nagappan:icse:2006}
- import relations~\cite{dualaekoko:esem:2009} the original is~\cite{schroeter:isese:2006}
- module control edges to another module~\cite{jiang:promise:2008}
- call dependency change counts~\cite{shin:msr:2009}
- higher co-changes correlated with failure~\cite{dambros:wcre:2009}

change failure:
- not changing frequently co-changed code might introduce errors~\cite{zimmermann:icse:2004}

% misc info \cite{moser:icse:2008} uses same metrics as Predicting Defects for Eclipse
% \cite{kim:icse:2007} uses metrics extracted from the APFEL tool

\section{Artifact Networks}
\label{chap:6:an}

- fixes occur at code peers~\cite{nguyen:icse:2010} 
- ego network measures on data and call dependencies build for one instance of program~\cite{zimmermann:icse:2008}
- ego binary netowrk can predict failure likelihood of binary~\cite{zimmermann:esem:2009}

\section{Technical Networks}
\label{chap:6:tn}
file failure:
- co changed files networks~\cite{meneely:fse:2008}

binary failure:
- ownership relations by changed same binary~\cite{pinzger:fse:2008}


\section{Discussion}
\label{chap:6:dis}
